
from datetime import datetime
from airflow.models import DAG
from airflow.operators.python_operator import PythonOperator
import redshift_connector
from airflow.providers.amazon.aws.operators.ecs import EcsRunTaskOperator
import json
import boto3


S3_BUCKET = 'demo-ml-datasets'
CLUSTER_NAME = "ECS-MWAA-cluster"
CONTAINER_NAME="mwaa-ecs-demo-container"
LAUNCH_TYPE = "FARGATE"


def get_ssm_params(name, parent_path="/mwaa/redshift"):
    ssm_client = boto3.client('ssm')
    return ssm_client.get_parameter(Name=f"{parent_path}/{name}")['Parameter']['Value']


def python_connector_redshift():
    secret_client = boto3.client('secretsmanager')
    response = secret_client.get_secret_value(
        SecretId=get_ssm_params("SECRET_ID")
    )
    secrets = json.loads(response['SecretString'])

    # Connect to the cluster

    with redshift_connector.connect(
        host=get_ssm_params("HOST"),
        database=get_ssm_params("DB"),
        port=get_ssm_params("PORT"),
        user=secrets['username'],
        password=secrets['password']) as conn:
        sql_params = json.loads(get_ssm_params("SQL_PARAMS"))
        schema = get_ssm_params("SCHEMA")
        table = get_ssm_params("TABLE")
        with conn.cursor() as cursor:
            cursor.execute(f'SELECT * FROM {schema}.{table} WHERE median_house_value > {sql_params["median_house_value"]} '
                           f'AND housing_median_age >= {sql_params["housing_median_age"]} AND '
                           f'population > {sql_params["population"]}')
            df = cursor.fetch_dataframe()
            print(df)


with DAG(
    dag_id=f"example_dag_redshift",
    schedule="@daily",
    start_date=datetime(2024, 6, 18),
    catchup=False
) as dag:
    client=boto3.client('ecs')
    services=client.list_services(cluster=CLUSTER_NAME,launchType=LAUNCH_TYPE)
    service=client.describe_services(cluster=CLUSTER_NAME,services=services['serviceArns'])
    my_python_task = PythonOperator(
        task_id="redshift_python_op", dag=dag, python_callable=python_connector_redshift,
    )

    run_ecs_task = EcsRunTaskOperator(
        task_id="run_ecs_task",
        dag=dag,
        cluster=CLUSTER_NAME,
        task_definition=service['services'][0]['taskDefinition'],
        launch_type=LAUNCH_TYPE,
        overrides={
            "containerOverrides": [
                {
                    "name": CONTAINER_NAME,
                    "command": ["echo hello world"],
                },
            ],
        },
        network_configuration=service['services'][0]['networkConfiguration'],
        awslogs_group="mwaa-ecs",
        awslogs_region="us-east-1",
        awslogs_stream_prefix=f"ecs/{CONTAINER_NAME}",
    )

    my_python_task >>  run_ecs_task
